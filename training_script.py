# -*- coding: utf-8 -*-
"""TicTakToe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DqZKreoAy-MfPc0mSoM8sT-IsSf5vIbH
"""

# WINCOMBINATIONS contains all winning combinations. The index of the strings correpsonds to grid positions.
# So for example, the winning combination 0b100010001 for X corresponds to index 0, 4, 8:
# GRID =  [["X", "", ""],
#          ["", "X", ""],
#          ["", "", "X"]]

WINCOMBINATIONS = [
  # rows
  0b000000111,
  0b000111000,
  0b111000000,
  # columns
  0b100100100,
  0b010010010,
  0b001001001,
  # diagonal
  0b100010001,
  0b001010100
 ]

# helper functions

# converts a base 10 number to base 3 string representation
def decimal_to_ternary(n):
  if n == 0:
      return '0'
  nums = []
  while n:
      n, r = divmod(n, 3)
      nums.append(str(r))
  return ''.join(reversed(nums))

def ternary_to_board(string):
  grid = [" "," "," "," "," "," "," "," "," "]
  i = 0
  for s in reversed(string):
    if s == "1":
      grid[i] = "X"
    elif s == "2":
      grid[i] = "O"
    i += 1
  return grid

import numpy as np
import random
import copy
import math

# Class responsible for updating Q Values and selecting best actions
class Learner:
  def __init__(self, side, discount_factor, epsilon):
     self.discount_factor = discount_factor
     self.epsilon = epsilon
     self.Q_Table = {0:{0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0}}
     self.K_Table = {0:{0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1}}
     self.side = side

  def get_epsilon(self):
    return epsilon

  def change_epsilon(self, epsilon):
    self.epsilon = epsilon

  # Given a new state, create a new row in Q_Table and K_Table by initializing all possible actions for that state
  # Q Values are initialized to 0
  # K Values are initialized to 1
  def add_new_entry(self, state):
    ternary_form = decimal_to_ternary(state)
    self.Q_Table[state] = {}
    self.K_Table[state] = {}

    for i in range(9-len(ternary_form)):
      self.Q_Table[state][i+len(ternary_form)] = 0
      self.K_Table[state][i+len(ternary_form)] = 1
        
    idx = 0
    for j in ternary_form: 
      if j == "0":
        self.Q_Table[state][len(ternary_form)-1-idx] = 0 
        self.K_Table[state][len(ternary_form)-1-idx] = 1 
      idx += 1

  # Updates the Q Value table. 
  # Note: state_prev is the current player's turn and state_after is the opponents' turn,
  # i.e. Q_Table should have no actions for state_after
  def update_table(self, state_prev, state_after, action, reward):
    #print(self.Q_Table)
    # If Q[s,a] doesn't exist, create a new entry in Q_Table and K_Table, see more in add_new_entry(self, state)
    if not self.Q_Table.__contains__(state_prev):
      self.add_new_entry(state_prev)
    #print(self.Q_Table)
    # If Q[s',a'] doesn't exist, create a new entry in Q_Table and K_Table, such entry in Q_Table only has
    # the action -1 to indicate it has no control over its opponent's turn.
    if not self.Q_Table.__contains__(state_after):
      self.Q_Table[state_after] = {-1:0}
    #print(self.Q_Table)
    # The future reward is 0 if s' has no possible actions
    if self.Q_Table[state_after] == {}:
      future_reward = 0
    else:
      future_reward = self.Q_Table[state_after][-1]
    #print(self.Q_Table)
    Q_prev = self.Q_Table[state_prev][action]
    learning_rate = 1 / self.K_Table[state_prev][action]

    # Update the Q_Table using the formula: Q[s,a] = Q[s,a] + α_k(r + γ * max{Q[s',a']}- Q[s,a])
    Q_new = Q_prev + learning_rate * (reward + self.discount_factor * future_reward - Q_prev)
    #Q_new = Q_prev + 0.1 * (reward + self.discount_factor * future_reward - Q_prev)
    self.Q_Table[state_prev][action] = Q_new

    # Update K_Table
    self.K_Table[state_prev][action] += 1
    return
  
  # Updates the Q_Table of the opponent.
  # Note: state_prev is the current player's turn and state_after is the opponents' turn,
  # hence Q[state_prev] has no actions (since we are working with the Q_Table of the opponent)
  def update_table_adversary(self, state_prev, state_after):
    if not self.Q_Table.__contains__(state_after):
      self.add_new_entry(state_after)

    # Q[state_prev][-1] has the value of max{ Q[state_after][a] }.
    # This assumes the Q Value of the opponent after you performed a move is equal to the
    # Q value of the opponent after it performs the best action response to yours
    best_move = self.state_best_moves(state_after)
    max_Q = self.Q_Table[state_after][best_move]
    self.Q_Table[state_prev] = {-1:max_Q}

  # Updates the Q_Table in end game states
  def update_table_adversary_end_game(self, state_prev, reward):
    self.Q_Table[state_prev] = {-1:reward}

  # Returns the action with max Q-Value for state
  def state_best_moves(self, state):
    if not self.Q_Table.__contains__(state):
      self.add_new_entry(state)
    return max(self.Q_Table[state], key=self.Q_Table[state].get)

  def get_Q_Table(self):
    return self.Q_Table
  
  def get_complete_strategy(self):
    ret = {}
    for state in self.Q_Table:
      ret[state] = self.state_best_moves(state)
    return ret
    
  def make_move(self, side, board):
    # return self.make_move_tau_softmax(side, board)
    return self.make_move_epsilon_greedy(side, board)

  # Chooses a random action for a state with probability ε, and the best action with proabability 1-ε
  def make_move_epsilon_greedy(self, side, board):
    state = board.get_state()
    choice = np.random.choice(["random", "best"], 1, p=[self.epsilon, 1-self.epsilon])
    if choice == "random":
      random_move = random.choice(board.get_all_legal_moves())
      return board.play_move(side, random_move)
    else:
      best_move = self.state_best_moves(state)
      return board.play_move(side, best_move)

  # # Chooses action with proability distribution based on Q-Values
  # def make_move_tau_softmax(self, side, board):
  #   if self.epsilon <= 0:
  #     tau = 0.01
  #   else:
  #     tau = epsilon
  #   state = board.get_state()
  #   sum = 0
  #   action_prob = []
  #   for action in self.Q_Table[state].keys():
  #     temp = math.exp(self.Q_Table[state][action]/self.epsilon)
  #     action_prob = np.append(action_prob, temp)
  #     sum += temp
  #   action_prob = [x / sum for x in action_prob]
  #   chosen_move = np.random.choice(list(self.Q_Table[state].keys()), 1, action_prob)
  #   return board.play_move(side, chosen_move[0])

class Board:
  def __init__(self):
    self.grid = [" "," "," "," "," "," "," "," "," "]

  def get_state(self):
    return self.convert_board_to_tenary()

  def convert_board_to_binary_X(self):
    sum = 0
    for i in range(9):
      if (self.grid[i] == "X"):
        sum += 2 ** i
    return sum

  def convert_board_to_binary_O(self):
    sum = 0
    for i in range(9):
      if (self.grid[i] == "O"):
        sum += 2 ** i
    return sum

  # Used to index Q-Values table
  def convert_board_to_tenary(self):
    sum = 0
    for i in range(9):
      if self.grid[i] == "X":
          cell = 1
      elif self.grid[i] == "O":
        cell = 2
      else:
        cell = 0
      sum += cell * (3 ** i)
    return sum    
  
  # Assumes move is legal (cell not yet occupied)
  def play_move(self, side, move):
    self.grid[move] = side
    return move

  # checks if a move is available to play
  def check_legal_move(self, move):
    return self.grid[move] == " "

  def get_all_legal_moves(self):
    legal_moves = []
    for i in range(9):
      if self.check_legal_move(i):
        legal_moves.append(i)
    return legal_moves

  def play_random_move(self, side):
    legal_moves = self.get_all_legal_moves()
    random_move = random.choice(legal_moves)
    return self.play_move(side, random_move)

  # converts the "X"s on the board to binary and compares it to the winning combinations
  def check_X_won(self):
    binary_representation = self.convert_board_to_binary_X()
    for winning_positions in WINCOMBINATIONS:
      if binary_representation & winning_positions == winning_positions:
        return True
    return False

  # converts the "O"s on the board to binary and compares it to the winning combinations
  def check_O_won(self):
    binary_representation = self.convert_board_to_binary_O()
    for winning_positions in WINCOMBINATIONS:
      if binary_representation & winning_positions == winning_positions:
        return True
    return False

  # checks if there are no more available actions to play
  def check_no_actions(self):
    for i in range(9):
      if self.grid[i] == " ":
        return False
    return True

  # returns the winner's side or draw if there are no more available actions to play, false otherwise
  def check_game_over(self):
    if self.check_X_won():
      return "X"
    if self.check_O_won():
      return "O"
    if self.check_no_actions():
      return "DRAW"
    return False

  def print_board(self):
    for i in range(3):
        print(("+" + "-" * 3) * 3 + "+")
        print("| "  + self.grid[i*3] + " " + "|" + " "  + self.grid[i*3+1]+ " " + "|" + " "  + self.grid[i*3+2] + " |")
    print(("+" + "-" * 3) * 3 + "+")

# The Beginner AI knows plays all moves randomly
class Beginner_AI:
  def __init__(self, side):
    self.side = side

  def make_move(self, side, board):
    return board.play_random_move(side) 

# The Intermmediate AI knows how to win and block the opponent from winning but plays other moves randomly
class Intermmediate_AI:
  def __init__(self, side):
    self.side = side
    self.moves_to_win_X = self.generate_winning_moves_X()
    self.moves_to_win_O = self.generate_winning_moves_O()

  def make_move(self, side, board):
    state = board.get_state()
    if side == "X":
      move_to_win = self.moves_to_win_X[state]
      move_to_block = self.moves_to_win_O[state]
      if move_to_win != None:
        return board.play_move("X", move_to_win)
      elif move_to_block != None:
        return board.play_move("X", move_to_block)
      else:
        return board.play_random_move("X") 
    else:
      move_to_win = self.moves_to_win_O[state]
      move_to_block = self.moves_to_win_X[state]
      if move_to_win != None:
        board.play_move("O", move_to_win)
      elif move_to_block != None:
        board.play_move("O", move_to_block)
      else:
        board.play_random_move("O") 
    
  # returns all states such that X can win with the next move
  def generate_winning_moves_X(self):
    ret = {}
    for i in range(19682):
      ternary_rep = decimal_to_ternary(i)
      state = ternary_to_board(ternary_rep)
      board = Board()
      board.grid = state
      for j in range(9):
        if board.check_legal_move(j):
          new_board = copy.deepcopy(board)
          new_board.play_move("X",j)
          if new_board.check_X_won() == True:
            ret[i] = j
            break
          else:
            ret[i] = None
    return ret

  # returns all states such that O can win with the next move
  def generate_winning_moves_O(self):
    ret = {}
    for i in range(19682):
      ternary_rep = decimal_to_ternary(i)
      state = ternary_to_board(ternary_rep)
      board = Board()
      board.grid = state
      for j in range(9):
        if board.check_legal_move(j):
          new_board = copy.deepcopy(board)
          new_board.play_move("O",j)
          if new_board.check_O_won() == True:
            ret[i] = j
            break
          else:
            ret[i] = None
    return ret

# Learner AI plays according to its Q Table
class Learner_AI:
  def __init__(self, side, learner):
    self.side = side
    self.learner = learner

  def make_move(self, side, board):
    state = board.get_state()
    best_move = self.learner.state_best_moves(state)
    board.play_move(side, best_move)  
    return best_move

def AI_vs_AI(player_X, player_O):
  board = Board()
  turn = "X"
  moves_played = []
  while True:
    if turn == "X":
      X_move = player_X.make_move("X", board)
      winner = board.check_game_over()
      moves_played.append(X_move)
      
      if winner == "DRAW":
        return ("DRAW",moves_played)
      elif winner == "X":
        return ("X",moves_played)

      turn  = "O"

    elif turn == "O":
      O_move = player_O.make_move("O", board)
      winner = board.check_game_over()
      moves_played.append(O_move)

      if winner == "DRAW":
        return ("DRAW",moves_played)
      elif winner == "O":
        return ("O",moves_played)        
      turn  = "X"
  return
  
class Trainer:
  def __init__(self, learner):
    self.total_iterations_trained = 0
    self.learner = learner
  
  def get_Q_Table(self):
    return self.learner.get_Q_Table()

  def get_complete_strategy(self):
    return self.learner.get_complete_strategy()

  def get_iterations_trained(self):
    return self.total_iterations_trained 

  def change_opponent(self, AI):
    self.AI = AI

  def train_num_times(self, opponent, iterations):
    for i in range(iterations):
      self.train_against_strategy(opponent)

  def test_learner(self, opponent):
    old_epsilon = self.learner.get_epsilon()
    # epsilon is changed to 0 so that the learner always chooses the best move
    self.learner.change_epsilon(0)
    X_wins = 0
    O_wins = 0
    draws = 0
    iter = 10000
    if self.learner.side == "X":
      player_X = self.learner
      player_O = opponent
    else:
      player_X = opponent    
      player_O = self.learner    
    for iter in range(iter):
      winner = AI_vs_AI(player_X,player_O)
      if winner[0] == "X":
        X_wins += 1
      elif winner[0] == "O":
        O_wins += 1
      else:
        draws += 1

    # restore epsilon
    self.learner.change_epsilon(old_epsilon)
      
    print("Generation "+ str(self.total_iterations_trained) + " learner:")
    print("X win rate:"+ str(X_wins/iter*100) +"%")
    print("O win rate:"+ str(O_wins/iter*100) +"%")
    print("Draw rate:"+ str(draws/iter*100) +"%")
    return X_wins/iter*100,O_wins/iter*100,draws/iter*100

  # train learner against an AI
  def train_against_strategy(self, opponent):
    self.total_iterations_trained += 1
    board = Board()
    learner = self.learner

    if learner.side == "X":
      player_X = learner
      player_O = opponent
    else:
      player_X = opponent
      player_O = learner

    turn = "X"

    while True:
      if turn == "X":
        prev_state = board.get_state()      
        X_move = player_X.make_move("X", board)
        winner = board.check_game_over()
        new_state = board.get_state()

        if winner == False:
          if learner.side == "X":
            player_X.update_table(prev_state, new_state, X_move, 0)
          else:
            player_O.update_table_adversary(prev_state, new_state)
        elif winner == "DRAW":
          if learner.side == "X":
            player_X.update_table(prev_state, new_state, X_move, 0.1)
          else:
            player_O.update_table_adversary_end_game(prev_state, 0.1)
          return

        elif winner == "X":
          if learner.side == "X":
            player_X.update_table(prev_state, new_state, X_move, 1)
          else:
            player_O.update_table_adversary_end_game(prev_state, -10)
          return

        turn  = "O"

      elif turn == "O":
        prev_state = board.get_state()      
        O_move = player_O.make_move("O", board)
        winner = board.check_game_over()
        new_state = board.get_state()

        if winner == False:
          if learner.side == "O":
            player_O.update_table(prev_state, new_state, O_move, 0)
          else:
            player_X.update_table_adversary(prev_state, new_state)

        elif winner == "DRAW":
          if learner.side == "O":
            player_O.update_table(prev_state, new_state, O_move, 0.1)
          else:
            player_X.update_table_adversary_end_game(prev_state, 0.1)
          return

        elif winner == "O":
          if learner.side == "O":
            player_O.update_table(prev_state, new_state, O_move, 1)
          else:
            player_X.update_table_adversary_end_game(prev_state, -10)
          return
        turn  = "X"
    return

import shutil
shutil.rmtree('/content/Player_X_AI', ignore_errors=True)
shutil.rmtree('/content/Player_O_AI', ignore_errors=True)

def save_learner(trainer, path):
  Q_Table = trainer.get_Q_Table()
  opt_strategy = trainer.get_complete_strategy()     
  new_generation = "Generation "+ str(trainer.total_iterations_trained)
  new_generation_path = os.path.join(path, new_generation)
  os.makedirs(new_generation_path)
  Q_Table_file_name = "Generation " + str(trainer.total_iterations_trained) + " Q Values"
  opt_strategy_file_name = "Generation " + str(trainer.total_iterations_trained) + " Best Strategy"
  os.chdir(new_generation_path)
  with open(Q_Table_file_name, 'w') as fp1:
      json.dump(Q_Table, fp1)
  with open(opt_strategy_file_name, 'w') as fp2:
      json.dump(opt_strategy, fp2)

class Stats:
  def __init__(self, beginner_AI, intermmediate_AI):
    self.beginner_AI = beginner_AI
    self.intermmediate_AI = intermmediate_AI
    self.win_rate_beginner = {}
    self.win_rate_intermmediate = {}
  
  def save_stats(self, trainer):
    print("Against Beginner_AI:")
    self.win_rate_beginner[trainer.total_iterations_trained] = trainer.test_learner(self.beginner_AI)
    print("Against Intermmediate_AI:")
    self.win_rate_intermmediate[trainer.total_iterations_trained] = trainer.test_learner(self.intermmediate_AI)

import json
import os
from google.colab import files
learner_X = Learner("X", 1, 1)
beginner_AI_O = Beginner_AI("O")
intermmediate_AI_O = Intermmediate_AI("O")
stats_X = Stats(beginner_AI_O, intermmediate_AI_O)
trainer_X = Trainer(learner_X)
epsilon = 1

while epsilon >= 0:
  for i in range(100):
    for j in range(25):
      trainer_X.train_num_times(beginner_AI_O, 1)
      trainer_X.train_num_times(intermmediate_AI_O, 3)
    save_learner(trainer_X, '/content/Player_X_AI')
    stats_X.save_stats(trainer_X)
  learner_X.change_epsilon(epsilon)
  epsilon -= 0.1

# os.chdir("/content")
# with open('stats_X_beginner.json', 'w') as fp1:
#     json.dump(stats_X.win_rate_beginner, fp1)
# with open('stats_X_intermmediate.json', 'w') as fp2:
#     json.dump(stats_X.win_rate_intermmediate, fp2)
!zip -r Player_X_AI.zip /content/Player_X_AI
files.download('Player_X_AI.zip')

learner_O = Learner("O", 1, 1)
beginner_AI_X = Beginner_AI("X")
intermmediate_AI_X = Intermmediate_AI("X")
stats_O = Stats(beginner_AI_X, intermmediate_AI_X)
trainer_O = Trainer(learner_O)
epsilon = 1

while epsilon >= 0:
  for i in range(100):
    for j in range(25):
      trainer_O.train_num_times(beginner_AI_X, 1)
      trainer_O.train_num_times(intermmediate_AI_X, 3)
    save_learner(trainer_O, '/content/Player_O_AI')
    stats_O.save_stats(trainer_O)
  learner_O.change_epsilon(epsilon)
  epsilon -= 0.1

os.chdir("/content")
with open('stats_O_beginner.json', 'w') as fp1:
    json.dump(stats_O.win_rate_beginner, fp1)
with open('stats_O_intermmediate.json', 'w') as fp2:
    json.dump(stats_O.win_rate_intermmediate, fp2)
!zip -r Player_O_AI.zip /content/Player_O_AI

trainer_O.test_learner(beginner_AI_X) 
trainer_O.test_learner(intermmediate_AI_X)

Simulator(learner_X,learner_O).start_game()

print(stats_X.win_rate_intermmediate)

import matplotlib.pylab as plt
a = {}
b = {}
c = {}
for k in stats_X.win_rate_beginner.keys():
  a[k] = stats_X.win_rate_beginner[k][0]
  b[k] = stats_X.win_rate_beginner[k][1]
  c[k] = stats_X.win_rate_beginner[k][2]
lists_1 = sorted(a.items()) 
x_a, y_a = zip(*lists_1) 
plt.plot(x_a, y_a)
lists_2 = sorted(b.items()) 
x_b, y_b = zip(*lists_2) 
plt.plot(x_b, y_b)
lists_3 = sorted(c.items()) 
x_c, y_c = zip(*lists_3) 
plt.plot(x_c, y_c)
plt.ylim([0, 100])
plt.show()

a = {}
b = {}
c = {}
for k in stats_X.win_rate_intermmediate.keys():
  a[k] = stats_X.win_rate_intermmediate[k][0]
  b[k] = stats_X.win_rate_intermmediate[k][1]
  c[k] = stats_X.win_rate_intermmediate[k][2]
lists_1 = sorted(a.items()) 
x_a, y_a = zip(*lists_1) 
plt.plot(x_a, y_a)
lists_2 = sorted(b.items()) 
x_b, y_b = zip(*lists_2) 
plt.plot(x_b, y_b)
lists_3 = sorted(c.items()) 
x_c, y_c = zip(*lists_3) 
plt.plot(x_c, y_c)
plt.ylim([0, 100])
plt.show()

import matplotlib.pylab as plt
a = {}
b = {}
c = {}
for k in stats_O.win_rate_beginner.keys():
  a[k] = stats_O.win_rate_beginner[k][0]
  b[k] = stats_O.win_rate_beginner[k][1]
  c[k] = stats_O.win_rate_beginner[k][2]
lists_1 = sorted(a.items()) 
x_a, y_a = zip(*lists_1) 
plt.plot(x_a, y_a)
lists_2 = sorted(b.items()) 
x_b, y_b = zip(*lists_2) 
plt.plot(x_b, y_b)
lists_3 = sorted(c.items()) 
x_c, y_c = zip(*lists_3) 
plt.plot(x_c, y_c)
plt.ylim([0, 100])
plt.show()

a = {}
b = {}
c = {}
for k in stats_O.win_rate_intermmediate.keys():
  a[k] = stats_O.win_rate_intermmediate[k][0]
  b[k] = stats_O.win_rate_intermmediate[k][1]
  c[k] = stats_O.win_rate_intermmediate[k][2]
lists_1 = sorted(a.items()) 
x_a, y_a = zip(*lists_1) 
plt.plot(x_a, y_a)
lists_2 = sorted(b.items()) 
x_b, y_b = zip(*lists_2) 
plt.plot(x_b, y_b)
lists_3 = sorted(c.items()) 
x_c, y_c = zip(*lists_3) 
plt.plot(x_c, y_c)
plt.ylim([0, 100])
plt.show()

# stats = (win_rate_beginner_X, win_rate_intermmediate_X, win_rate_beginner_O, win_rate_intermmediate_O)
# with open('trainer_X_data.pkl', 'wb') as outp:
#     pickle.dump(trainer_X, outp, pickle.HIGHEST_PROTOCOL)
# with open('trainer_O_data.pkl', 'wb') as outp:
#     pickle.dump(trainer_O, outp, pickle.HIGHEST_PROTOCOL)
# with open('learner_X_data.pkl', 'wb') as outp:
#     pickle.dump(learner_X, outp, pickle.HIGHEST_PROTOCOL)
# with open('learner_O_data.pkl', 'wb') as outp:
#     pickle.dump(learner_O, outp, pickle.HIGHEST_PROTOCOL)
# with open('stats.pkl', 'wb') as outp:
#     pickle.dump(stats, outp, pickle.HIGHEST_PROTOCOL)

import pickle
with open('trainer_X_data.pkl', 'rb') as inp:
    trainer_X = pickle.load(inp)
with open('trainer_O_data.pkl', 'rb') as inp:
    trainer_O = pickle.load(inp)
with open('learner_X_data.pkl', 'rb') as inp:
    learner_X = pickle.load(inp)
with open('learner_O_data.pkl', 'rb') as inp:
    learner_O = pickle.load(inp)
with open('stats.pkl', 'rb') as inp:
    stats = pickle.load(inp)
win_rate_beginner_X, win_rate_intermmediate_X, win_rate_beginner_O, win_rate_intermmediate_O = stats

class Simulator:
  def __init__(self, learner_X, learner_O):
    self.learner_X = learner_X
    self.learner_O = learner_O
    self.board = Board()

  def get_move(self, turn):
    if turn == "X":
      if self.player_side == "X":
        move = input("Choose a move: ")
        move = int(move)
        assert self.board.check_legal_move(move)
      else:
        move = self.learner_X.state_best_moves(self.board.get_state())
    elif turn == "O":
      if self.player_side == "O":
        move = input("Choose a move: ")
        move = int(move)
        assert self.board.check_legal_move(int(move))
      else:
        move = self.learner_O.state_best_moves(self.board.get_state())

    return move

  def start_game(self):
    self.player_side = input("Choose your side (X/O): ")
    assert self.player_side == "X" or self.player_side == "O"
    
    turn = "X"
    self.board.print_board()
    while True:
      if turn == "X":
        X_move = self.get_move("X")
        self.board.play_move("X", X_move)  
        print("New Board:")
        self.board.print_board()

        winner = self.board.check_game_over()

        if winner == "DRAW":
          print("Game is a draw")
          return
        elif winner == "X":
          print("X wins")
          return

        turn  = "O"

      elif turn == "O":
        O_move = self.get_move("O")
        self.board.play_move("O", O_move)  
        print("New Board:")
        self.board.print_board()

        winner = self.board.check_game_over()

        if winner == "DRAW":
          print("Game is a draw")
          return
        elif winner == "O":
          print("O wins")
          return
          
        turn  = "X"
    return